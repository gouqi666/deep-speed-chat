[2023-04-21 15:29:04,405] [WARNING] [runner.py:190:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-04-21 15:29:04,470] [INFO] [runner.py:540:main] cmd = /home/wzt/miniconda3/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None main.py --actor_model_name_or_path /nfs/wzt/DeepSpeedExamples/applications/DeepSpeed-Chat/output/actor-models/1.3b --critic_model_name_or_path /nfs/wzt/DeepSpeedExamples/applications/DeepSpeed-Chat/output/reward-models/350m --actor_zero_stage 0 --critic_zero_stage 0 --num_padding_at_beginning 1 --gradient_accumulation_steps 2 --deepspeed --actor_lora_dim 128 --enable_hybrid_engine --actor_gradient_checkpointing --output_dir /nfs/wzt/DeepSpeedExamples/applications/DeepSpeed-Chat/output/step3-models/1.3b
[2023-04-21 15:29:06,122] [INFO] [launch.py:229:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2023-04-21 15:29:06,122] [INFO] [launch.py:235:main] nnodes=1, num_local_procs=8, node_rank=0
[2023-04-21 15:29:06,122] [INFO] [launch.py:246:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2023-04-21 15:29:06,122] [INFO] [launch.py:247:main] dist_world_size=8
[2023-04-21 15:29:06,123] [INFO] [launch.py:249:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2023-04-21 15:29:12,406] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 627.37it/s]
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 708.14it/s]
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 672.54it/s]
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 687.25it/s]
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
100%|██████████| 2/2 [00:00<00:00, 686.69it/s]
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 681.50it/s]
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 709.64it/s]
Found cached dataset parquet (/home/wzt/.cache/huggingface/datasets/Dahoas___parquet/default-b9d2c4937d617106/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
  0%|          | 0/2 [00:00<?, ?it/s]100%|██████████| 2/2 [00:00<00:00, 681.56it/s]
************************[start] Initializing Actor Model [start] *************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wzt/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.5854036808013916 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wzt/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.4729321002960205 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wzt/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.49913501739501953 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Loading extension module fused_adam...
Time to load fused_adam op: 0.2036302089691162 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.21217751502990723 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.3030674457550049 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.4158353805541992 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wzt/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.6829748153686523 seconds
[2023-04-21 15:30:26,990] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1+4de4d2ac, git-hash=4de4d2ac, git-branch=master
[2023-04-21 15:30:41,853] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-04-21 15:30:41,855] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-04-21 15:30:41,855] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-04-21 15:30:41,884] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-04-21 15:30:41,884] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-04-21 15:30:41,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-04-21 15:30:41,981] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-04-21 15:30:41,982] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f195180b520>
[2023-04-21 15:30:41,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...

[2023-04-21 15:30:41,983] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-04-21 15:30:41,983] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-04-21 15:30:41,983] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   amp_params ................... False
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f19487dc9d0>
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-04-21 15:30:41,984] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   dump_state ................... False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1}
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-04-21 15:30:41,985] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   fp16_enabled ................. True
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 2
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   gradient_clipping ............ 1.0
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=True max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-04-21 15:30:41,986] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-04-21 15:30:41,987] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   pld_params ................... False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   train_batch_size ............. 256
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  16
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   world_size ................... 8
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   zero_enabled ................. False
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-04-21 15:30:41,988] [INFO] [config.py:957:print]   zero_optimization_stage ...... 0
[2023-04-21 15:30:41,988] [INFO] [config.py:943:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 16, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "offload_param": {
            "device": "none"
        }, 
        "offload_optimizer": {
            "device": "none"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale_window": 100
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": true, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }
}
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Emitting ninja build file /home/wzt/.cache/torch_extensions/py39_cu117/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.19447636604309082 seconds
Loading extension module utils...
Time to load utils op: 0.2018289566040039 seconds
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 0.20167064666748047 seconds
Time to load utils op: 0.20190858840942383 seconds
Time to load utils op: 0.2018752098083496 seconds
Time to load utils op: 0.20219206809997559 seconds
Loading extension module utils...
Time to load utils op: 0.2016911506652832 seconds
Loading extension module utils...
Time to load utils op: 0.2026362419128418 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Creating extension directory /home/wzt/.cache/torch_extensions/py39_cu117/transformer_inference...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wzt/.cache/torch_extensions/py39_cu117/transformer_inference/build.ninja...
Building extension module transformer_inference...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[1/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o 
[2/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o 
[3/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o 
[4/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o 
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(57): warning #177-D: variable "lane" was declared but never referenced

/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(94): warning #177-D: variable "half_dim" was declared but never referenced

/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(111): warning #177-D: variable "vals_half" was declared but never referenced

/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(112): warning #177-D: variable "output_half" was declared but never referenced

/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(129): warning #177-D: variable "lane" was declared but never referenced

[5/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o 
[6/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o 
[7/9] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o 
[8/9] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/TH -isystem /home/wzt/miniconda3/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /home/wzt/miniconda3/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -c /home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o 
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = float]’:
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1750:99:   required from here
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                                       k * InferenceContext::Instance().GetMaxTokenLenght(),
                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                           k * InferenceContext::Instance().GetMaxTokenLenght(),
                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = __half]’:
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1753:49:   required from here
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                                       k * InferenceContext::Instance().GetMaxTokenLenght(),
                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLenght(),
                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
                           k * InferenceContext::Instance().GetMaxTokenLenght(),
                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
/home/wzt/miniconda3/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLenght())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ inside { } [-Wnarrowing]
[9/9] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o -shared -lcurand -L/home/wzt/miniconda3/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o transformer_inference.so
Loading extension module transformer_inference...
Time to load transformer_inference op: 23.398571491241455 seconds
Loading extension module transformer_inference...
Time to load transformer_inference op: 23.376818418502808 seconds
[2023-04-21 15:31:05,692] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 2048, 'intermediate_size': 8192, 'heads': 32, 'num_hidden_layers': -1, 'fp16': True, 'pre_layer_norm': True, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'q_int8': False, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': -1, 'rotate_half': False, 'rotate_every_two': True, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.ReLU: 2>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 512, 'min_out_tokens': 512, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': True, 'transposed_mode': True}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Loading extension module transformer_inference...
Time to load transformer_inference op: 23.38142156600952 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Loading extension module transformer_inference...
Loading extension module transformer_inference...
Time to load transformer_inference op: 23.464787244796753 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Time to load transformer_inference op: 23.464281797409058 seconds
Loading extension module transformer_inference...
Loading extension module transformer_inference...
Time to load transformer_inference op: 23.464889764785767 seconds
Time to load transformer_inference op: 23.465088605880737 seconds
Loading extension module transformer_inference...
Time to load transformer_inference op: 23.467466831207275 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
Time to load transformer_inference op: 0.03079962730407715 seconds	- Avoid using `tokenizers` before the fork if possible

	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.050791263580322266 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.03802037239074707 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.038526296615600586 seconds
Time to load transformer_inference op: 0.0386810302734375 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.038858890533447266 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.03884005546569824 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.05860710144042969 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.05100440979003906 seconds
******************[end] Initialized Actor Model [end] (duration: 71.74s)******************
*************************[start] Initializing Ref Model [start] **************************
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.03381919860839844 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Time to load transformer_inference op: 0.038718461990356445 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.04147911071777344 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.040429115295410156 seconds
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.04031658172607422 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.0417482852935791 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module transformer_inference, skipping build step...
Loading extension module transformer_inference...
Time to load transformer_inference op: 0.06098794937133789 seconds
[2023-04-21 15:31:17,680] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1+4de4d2ac, git-hash=4de4d2ac, git-branch=master
[2023-04-21 15:31:23,558] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-04-21 15:31:23,573] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   amp_params ................... False
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f193e89e580>
[2023-04-21 15:31:23,573] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   dump_state ................... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   fp16_enabled ................. True
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 2
[2023-04-21 15:31:23,574] [INFO] [config.py:957:print]   gradient_clipping ............ 1.0
[2023-04-21 15:31:23,586] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-04-21 15:31:23,586] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-04-21 15:31:23,586] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-04-21 15:31:23,586] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   pld_params ................... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   train_batch_size ............. 256
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  16
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   world_size ................... 8
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   zero_enabled ................. False
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-04-21 15:31:23,587] [INFO] [config.py:957:print]   zero_optimization_stage ...... 0
[2023-04-21 15:31:23,587] [INFO] [config.py:943:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 16, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "offload_param": {
            "device": "none"
        }, 
        "memory_efficient_linear": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false
}
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006840229034423828 seconds
*******************[end] Initialized Ref Model [end] (duration: 17.57s)*******************
************************[start] Initializing Critic Model [start] ************************
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009577274322509766 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0010066032409667969 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008292198181152344 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0013318061828613281 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008859634399414062 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009009838104248047 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.000873565673828125 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0010180473327636719 seconds
[2023-04-21 15:31:28,622] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1+4de4d2ac, git-hash=4de4d2ac, git-branch=master
[2023-04-21 15:31:28,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-04-21 15:31:28,752] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-04-21 15:31:28,752] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-04-21 15:31:28,764] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-04-21 15:31:28,764] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[2023-04-21 15:31:28,780] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-04-21 15:31:28,780] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-04-21 15:31:28,781] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f193e7e3550>
[2023-04-21 15:31:28,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:31:28,781] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   amp_params ................... False
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f188f4abdf0>
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-04-21 15:31:28,782] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   dump_state ................... False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'min_scale': 1}
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-04-21 15:31:28,783] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   fp16_enabled ................. True
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 2
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   gradient_clipping ............ 1.0
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-04-21 15:31:28,784] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   pld_params ................... False
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-04-21 15:31:28,785] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   train_batch_size ............. 256
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  16
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   world_size ................... 8
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=False
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   zero_enabled ................. False
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-04-21 15:31:28,786] [INFO] [config.py:957:print]   zero_optimization_stage ...... 0
[2023-04-21 15:31:28,787] [INFO] [config.py:943:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 16, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "offload_param": {
            "device": "none"
        }, 
        "offload_optimizer": {
            "device": "none"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale_window": 100
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }
}
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004899501800537109 seconds
******************[end] Initialized Critic Model [end] (duration: 5.20s)******************
************************[start] Initializing Reward Model [start] ************************
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0009570121765136719 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0004284381866455078 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0012047290802001953 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0010924339294433594 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0013360977172851562 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0015425682067871094 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0011224746704101562 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0007002353668212891 seconds
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module fused_adam, skipping build step...
Loading extension module fused_adam...
Time to load fused_adam op: 0.0012052059173583984 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006680488586425781 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006630420684814453 seconds
[2023-04-21 15:31:38,096] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.1+4de4d2ac, git-hash=4de4d2ac, git-branch=master
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0010921955108642578 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009920597076416016 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009148120880126953 seconds
[2023-04-21 15:31:38,817] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-04-21 15:31:38,818] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[2023-04-21 15:31:38,818] [INFO] [config.py:957:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-04-21 15:31:38,818] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-04-21 15:31:38,818] [INFO] [config.py:957:print]   amp_enabled .................. False
[2023-04-21 15:31:38,818] [INFO] [config.py:957:print]   amp_params ................... False
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[2023-04-21 15:31:38,818] [INFO] [config.py:957:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f19487e23d0>
Time to load utils op: 0.0007040500640869141 seconds
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   communication_data_type ...... None
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   disable_allgather ............ False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   dump_state ................... False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... None
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[2023-04-21 15:31:38,819] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   fp16_enabled ................. True
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   global_rank .................. 0
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 2
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   gradient_clipping ............ 1.0
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 65536
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[2023-04-21 15:31:38,820] [INFO] [config.py:957:print]   loss_scale ................... 0
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   memory_breakdown ............. False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   optimizer_name ............... None
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   optimizer_params ............. None
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   pld_enabled .................. False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   pld_params ................... False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   prescale_gradients ........... False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   scheduler_name ............... None
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   scheduler_params ............. None
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   sparse_attention ............. None
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   steps_per_print .............. 10
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   train_batch_size ............. 256
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  16
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[2023-04-21 15:31:38,821] [INFO] [config.py:957:print]   world_size ................... 8
[2023-04-21 15:31:38,822] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[2023-04-21 15:31:38,822] [INFO] [config.py:957:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=False
[2023-04-21 15:31:38,822] [INFO] [config.py:957:print]   zero_enabled ................. False
[2023-04-21 15:31:38,822] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[2023-04-21 15:31:38,822] [INFO] [config.py:957:print]   zero_optimization_stage ...... 0
[2023-04-21 15:31:38,822] [INFO] [config.py:943:print_user_config]   json = {
    "train_batch_size": 256, 
    "train_micro_batch_size_per_gpu": 16, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 0, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "offload_param": {
            "device": "none"
        }, 
        "memory_efficient_linear": false
    }, 
    "fp16": {
        "enabled": true
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false
}
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0007352828979492188 seconds
*****************[end] Initialized Reward Model [end] (duration: 10.04s)******************
***** Running training *****
Beginning of Epoch 1/1, Total Generation Batches 120
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00034046173095703125 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00033736228942871094 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00032448768615722656 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0011990070343017578 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00038433074951171875 seconds
Using /home/wzt/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.00033020973205566406 seconds
------------------------------------------------------
Free memory : 62.050354 (GigaBytes)  
Total memory: 79.210449 (GigaBytes)  
Requested memory: 2.062500 (GigaBytes) 
Setting maximum total tokens (input + output) to 512 
WorkSpace: 0x7f100a000000 
------------------------------------------------------
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
/home/wzt/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2324: UserWarning: Specified kernel cache directory could not be created! This disables kernel caching. Specified directory is /home/wzt/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/cuda/jit_utils.cpp:1443.)
  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)
epoch: 0|step: 0|ppo_ep: 1|act_loss: 0.38134765625|cri_loss: 0.25390625|unsuper_loss: 0.0
average reward score: 7.2421875
-------------------------------------------------------------------------------------
|E2E latency=4.42s |Gather latency=0.00s (0.00%) |Generate time=2.92s (66.00%) |Training time=1.25s (28.19%) |Others=0.26 (5.81%)|CurSamplesPerSec=28.94 |AvgSamplesPerSec=28.94
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,064] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,064] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536, reducing to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
[2023-04-21 15:31:50,065] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 65536 to 32768.0
epoch: 0|step: 1|ppo_ep: 1|act_loss: 0.41943359375|cri_loss: 0.287109375|unsuper_loss: 0.0
average reward score: 7.15625
-------------------------------------------------------------------------------------
|E2E latency=3.16s |Gather latency=0.00s (0.00%) |Generate time=1.36s (43.13%) |Training time=1.58s (50.01%) |Others=0.22 (6.86%)|CurSamplesPerSec=40.51 |AvgSamplesPerSec=33.76
epoch: 0|step: 2|ppo_ep: 1|act_loss: 0.51220703125|cri_loss: 0.351318359375|unsuper_loss: 0.0
average reward score: 7.30078125
-------------------------------------------------------------------------------------
|E2E latency=2.84s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.88%) |Training time=1.25s (43.99%) |Others=0.23 (8.12%)|CurSamplesPerSec=45.15 |AvgSamplesPerSec=36.86
[2023-04-21 15:31:55,825] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,825] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 1
[2023-04-21 15:31:55,826] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[2023-04-21 15:31:55,826] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
epoch: 0|step: 3|ppo_ep: 1|act_loss: 0.482177734375|cri_loss: 0.32666015625|unsuper_loss: 0.0
average reward score: 7.43359375
-------------------------------------------------------------------------------------
|E2E latency=2.93s |Gather latency=0.00s (0.00%) |Generate time=1.36s (46.38%) |Training time=1.35s (46.11%) |Others=0.22 (7.51%)|CurSamplesPerSec=43.75 |AvgSamplesPerSec=38.37
epoch: 0|step: 4|ppo_ep: 1|act_loss: 0.55419921875|cri_loss: 0.386962890625|unsuper_loss: 0.0
average reward score: 7.16015625
-------------------------------------------------------------------------------------
|E2E latency=2.84s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.89%) |Training time=1.25s (44.13%) |Others=0.23 (7.98%)|CurSamplesPerSec=45.08 |AvgSamplesPerSec=39.55
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[2023-04-21 15:32:01,600] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[2023-04-21 15:32:01,600] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 2
[2023-04-21 15:32:01,601] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
epoch: 0|step: 5|ppo_ep: 1|act_loss: 0.462890625|cri_loss: 0.3134765625|unsuper_loss: 0.0
average reward score: 7.51953125
-------------------------------------------------------------------------------------
|E2E latency=2.94s |Gather latency=0.00s (0.00%) |Generate time=1.36s (46.22%) |Training time=1.36s (46.35%) |Others=0.22 (7.43%)|CurSamplesPerSec=43.61 |AvgSamplesPerSec=40.17
epoch: 0|step: 6|ppo_ep: 1|act_loss: 0.416015625|cri_loss: 0.292724609375|unsuper_loss: 0.0
average reward score: 7.2578125
-------------------------------------------------------------------------------------
|E2E latency=2.84s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.91%) |Training time=1.25s (44.08%) |Others=0.23 (8.02%)|CurSamplesPerSec=45.07 |AvgSamplesPerSec=40.81
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 3
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
[2023-04-21 15:32:07,375] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
epoch: 0|step: 7|ppo_ep: 1|act_loss: 0.63720703125|cri_loss: 0.439453125|unsuper_loss: 0.0
average reward score: 7.06640625
-------------------------------------------------------------------------------------
|E2E latency=2.93s |Gather latency=0.00s (0.00%) |Generate time=1.36s (46.34%) |Training time=1.36s (46.26%) |Others=0.22 (7.40%)|CurSamplesPerSec=43.62 |AvgSamplesPerSec=41.14
epoch: 0|step: 8|ppo_ep: 1|act_loss: 0.432373046875|cri_loss: 0.28759765625|unsuper_loss: 0.0
average reward score: 7.18359375
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.72%) |Training time=1.25s (44.00%) |Others=0.24 (8.28%)|CurSamplesPerSec=44.95 |AvgSamplesPerSec=41.53
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:362:_update_scale] 
Grad overflow on iteration 4
[2023-04-21 15:32:13,161] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
epoch: 0|step: 9|ppo_ep: 1|act_loss: 0.50732421875|cri_loss: 0.35400390625|unsuper_loss: 0.0[2023-04-21 15:32:13,162] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0

[2023-04-21 15:32:13,162] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
[2023-04-21 15:32:13,162] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
average reward score: 7.2265625
-------------------------------------------------------------------------------------
|E2E latency=2.94s |Gather latency=0.00s (0.00%) |Generate time=1.36s (46.33%) |Training time=1.36s (46.33%) |Others=0.22 (7.34%)|CurSamplesPerSec=43.55 |AvgSamplesPerSec=41.72
epoch: 0|step: 10|ppo_ep: 1|act_loss: 0.525390625|cri_loss: 0.37109375|unsuper_loss: 0.0
average reward score: 7.37890625
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.56%) |Training time=1.25s (43.91%) |Others=0.24 (8.53%)|CurSamplesPerSec=44.84 |AvgSamplesPerSec=41.99
epoch: 0|step: 11|ppo_ep: 1|act_loss: 0.447265625|cri_loss: 0.2978515625|unsuper_loss: 0.0
average reward score: 7.3046875
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.62%) |Training time=1.37s (46.10%) |Others=0.25 (8.27%)|CurSamplesPerSec=42.96 |AvgSamplesPerSec=42.07
epoch: 0|step: 12|ppo_ep: 1|act_loss: 0.552734375|cri_loss: 0.380859375|unsuper_loss: 0.0
average reward score: 7.2734375
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.52%) |Training time=1.26s (44.00%) |Others=0.24 (8.49%)|CurSamplesPerSec=44.84 |AvgSamplesPerSec=42.27
epoch: 0|step: 13|ppo_ep: 1|act_loss: 0.45751953125|cri_loss: 0.302978515625|unsuper_loss: 0.0
average reward score: 7.125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.62%) |Training time=1.37s (46.04%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.96 |AvgSamplesPerSec=42.32
epoch: 0|step: 14|ppo_ep: 1|act_loss: 0.5205078125|cri_loss: 0.357177734375|unsuper_loss: 0.0
average reward score: 7.33984375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.64%) |Training time=1.26s (43.93%) |Others=0.24 (8.43%)|CurSamplesPerSec=44.80 |AvgSamplesPerSec=42.47
epoch: 0|step: 15|ppo_ep: 1|act_loss: 0.6044921875|cri_loss: 0.423095703125|unsuper_loss: 0.0
average reward score: 7.7109375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.62%) |Training time=1.37s (46.03%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.00 |AvgSamplesPerSec=42.51
epoch: 0|step: 16|ppo_ep: 1|act_loss: 0.431640625|cri_loss: 0.29345703125|unsuper_loss: 0.0
average reward score: 7.03125
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.70%) |Training time=1.26s (44.06%) |Others=0.23 (8.23%)|CurSamplesPerSec=44.86 |AvgSamplesPerSec=42.64
epoch: 0|step: 17|ppo_ep: 1|act_loss: 0.470703125|cri_loss: 0.30419921875|unsuper_loss: 0.0
average reward score: 7.2421875
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.70%) |Training time=1.36s (45.94%) |Others=0.25 (8.36%)|CurSamplesPerSec=43.10 |AvgSamplesPerSec=42.66
epoch: 0|step: 18|ppo_ep: 1|act_loss: 0.5283203125|cri_loss: 0.3671875|unsuper_loss: 0.0
average reward score: 7.32421875
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.71%) |Training time=1.26s (44.06%) |Others=0.23 (8.23%)|CurSamplesPerSec=44.90 |AvgSamplesPerSec=42.78
[2023-04-21 15:32:42,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[9.65e-07, 9.65e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:32:42,078] [INFO] [timer.py:199:stop] epoch=0/micro_step=20/global_step=10, RunningAvgSamplesPerSec=164.60080717861203, CurrSamplesPerSec=164.4141298464257, MemAllocated=16.18GB, MaxMemAllocated=33.09GB
[2023-04-21 15:32:42,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=5, lr=[2.5000000000000004e-07, 2.5000000000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
epoch: 0|step: 19|ppo_ep: 1|act_loss: 0.45263671875|cri_loss: 0.3212890625|unsuper_loss: 0.0
average reward score: 7.359375
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.64%) |Training time=1.37s (46.01%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.03 |AvgSamplesPerSec=42.79
epoch: 0|step: 20|ppo_ep: 1|act_loss: 0.5556640625|cri_loss: 0.37646484375|unsuper_loss: 0.0
average reward score: 7.453125
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.61%) |Training time=1.26s (44.06%) |Others=0.24 (8.33%)|CurSamplesPerSec=44.86 |AvgSamplesPerSec=42.88
epoch: 0|step: 21|ppo_ep: 1|act_loss: 0.46240234375|cri_loss: 0.30322265625|unsuper_loss: 0.0
average reward score: 7.19921875
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.78%) |Training time=1.36s (45.92%) |Others=0.25 (8.30%)|CurSamplesPerSec=43.07 |AvgSamplesPerSec=42.89
epoch: 0|step: 22|ppo_ep: 1|act_loss: 0.49267578125|cri_loss: 0.34326171875|unsuper_loss: 0.0
average reward score: 7.3125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.62%) |Training time=1.26s (43.96%) |Others=0.24 (8.42%)|CurSamplesPerSec=44.80 |AvgSamplesPerSec=42.97
epoch: 0|step: 23|ppo_ep: 1|act_loss: 0.348876953125|cri_loss: 0.227783203125|unsuper_loss: 0.0
average reward score: 7.171875
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.77%) |Training time=1.36s (45.85%) |Others=0.25 (8.37%)|CurSamplesPerSec=43.12 |AvgSamplesPerSec=42.98
epoch: 0|step: 24|ppo_ep: 1|act_loss: 0.42333984375|cri_loss: 0.28369140625|unsuper_loss: 0.0
average reward score: 7.2578125
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.56%) |Training time=1.26s (44.02%) |Others=0.24 (8.42%)|CurSamplesPerSec=44.86 |AvgSamplesPerSec=43.05
epoch: 0|step: 25|ppo_ep: 1|act_loss: 0.5712890625|cri_loss: 0.40185546875|unsuper_loss: 0.0
average reward score: 7.1015625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.62%) |Training time=1.37s (46.02%) |Others=0.25 (8.36%)|CurSamplesPerSec=43.00 |AvgSamplesPerSec=43.05
epoch: 0|step: 26|ppo_ep: 1|act_loss: 0.51806640625|cri_loss: 0.36279296875|unsuper_loss: 0.0
average reward score: 7.109375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.44%) |Training time=1.26s (43.95%) |Others=0.25 (8.62%)|CurSamplesPerSec=44.78 |AvgSamplesPerSec=43.11
epoch: 0|step: 27|ppo_ep: 1|act_loss: 0.45458984375|cri_loss: 0.30908203125|unsuper_loss: 0.0
average reward score: 7.30859375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.52%) |Training time=1.37s (46.12%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.01 |AvgSamplesPerSec=43.10
epoch: 0|step: 28|ppo_ep: 1|act_loss: 0.4345703125|cri_loss: 0.2939453125|unsuper_loss: 0.0
average reward score: 7.0859375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.47%) |Training time=1.26s (44.05%) |Others=0.24 (8.48%)|CurSamplesPerSec=44.82 |AvgSamplesPerSec=43.16
epoch: 0|step: 29|ppo_ep: 1|act_loss: 0.533203125|cri_loss: 0.36865234375|unsuper_loss: 0.0
average reward score: 7.2578125
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.54%) |Training time=1.37s (46.11%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.03 |AvgSamplesPerSec=43.16
epoch: 0|step: 30|ppo_ep: 1|act_loss: 0.548828125|cri_loss: 0.384521484375|unsuper_loss: 0.0
average reward score: 7.4296875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.40%) |Training time=1.26s (43.88%) |Others=0.25 (8.72%)|CurSamplesPerSec=44.73 |AvgSamplesPerSec=43.21
epoch: 0|step: 31|ppo_ep: 1|act_loss: 0.394775390625|cri_loss: 0.270751953125|unsuper_loss: 0.0
average reward score: 7.203125
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.62%) |Training time=1.37s (46.03%) |Others=0.25 (8.36%)|CurSamplesPerSec=43.08 |AvgSamplesPerSec=43.20
epoch: 0|step: 32|ppo_ep: 1|act_loss: 0.395263671875|cri_loss: 0.2822265625|unsuper_loss: 0.0
average reward score: 7.22265625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.42%) |Training time=1.26s (43.97%) |Others=0.25 (8.61%)|CurSamplesPerSec=44.77 |AvgSamplesPerSec=43.25
epoch: 0|step: 33|ppo_ep: 1|act_loss: 0.58056640625|cri_loss: 0.40966796875|unsuper_loss: 0.0
average reward score: 7.5703125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.43%) |Training time=1.38s (46.21%) |Others=0.25 (8.36%)|CurSamplesPerSec=42.91 |AvgSamplesPerSec=43.24
epoch: 0|step: 34|ppo_ep: 1|act_loss: 0.41162109375|cri_loss: 0.2705078125|unsuper_loss: 0.0
average reward score: 7.2734375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.46%) |Training time=1.26s (44.06%) |Others=0.24 (8.49%)|CurSamplesPerSec=44.83 |AvgSamplesPerSec=43.28
epoch: 0|step: 35|ppo_ep: 1|act_loss: 0.416015625|cri_loss: 0.27001953125|unsuper_loss: 0.0
average reward score: 7.265625
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.58%) |Training time=1.37s (46.09%) |Others=0.25 (8.34%)|CurSamplesPerSec=43.04 |AvgSamplesPerSec=43.28
epoch: 0|step: 36|ppo_ep: 1|act_loss: 0.398681640625|cri_loss: 0.283935546875|unsuper_loss: 0.0
average reward score: 7.24609375
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.51%) |Training time=1.26s (44.09%) |Others=0.24 (8.40%)|CurSamplesPerSec=44.88 |AvgSamplesPerSec=43.32
epoch: 0|step: 37|ppo_ep: 1|act_loss: 0.453857421875|cri_loss: 0.310791015625|unsuper_loss: 0.0
average reward score: 7.671875
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.65%) |Training time=1.37s (46.00%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.03 |AvgSamplesPerSec=43.31
epoch: 0|step: 38|ppo_ep: 1|act_loss: 0.2159423828125|cri_loss: 0.1563720703125|unsuper_loss: 0.0
average reward score: 7.19921875
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.48%) |Training time=1.26s (44.13%) |Others=0.24 (8.39%)|CurSamplesPerSec=44.86 |AvgSamplesPerSec=43.35
[2023-04-21 15:33:40,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.93e-06, 1.93e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:33:40,391] [INFO] [timer.py:199:stop] epoch=0/micro_step=40/global_step=20, RunningAvgSamplesPerSec=164.3549467325197, CurrSamplesPerSec=163.20812138915906, MemAllocated=16.18GB, MaxMemAllocated=33.09GB
[2023-04-21 15:33:40,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=5, lr=[7.5e-07, 7.5e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
epoch: 0|step: 39|ppo_ep: 1|act_loss: 0.33544921875|cri_loss: 0.2381591796875|unsuper_loss: 0.0
average reward score: 7.328125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.44%) |Training time=1.38s (46.23%) |Others=0.25 (8.33%)|CurSamplesPerSec=42.93 |AvgSamplesPerSec=43.34
epoch: 0|step: 40|ppo_ep: 1|act_loss: 0.23779296875|cri_loss: 0.171630859375|unsuper_loss: 0.0
average reward score: 7.16015625
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.64%) |Training time=1.26s (44.09%) |Others=0.24 (8.27%)|CurSamplesPerSec=44.86 |AvgSamplesPerSec=43.37
epoch: 0|step: 41|ppo_ep: 1|act_loss: 0.326416015625|cri_loss: 0.227294921875|unsuper_loss: 0.0
average reward score: 7.36328125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.50%) |Training time=1.38s (46.15%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.92 |AvgSamplesPerSec=43.36
epoch: 0|step: 42|ppo_ep: 1|act_loss: 0.183349609375|cri_loss: 0.131591796875|unsuper_loss: 0.0
average reward score: 7.32421875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.54%) |Training time=1.26s (43.99%) |Others=0.24 (8.46%)|CurSamplesPerSec=44.77 |AvgSamplesPerSec=43.39
epoch: 0|step: 43|ppo_ep: 1|act_loss: 0.173828125|cri_loss: 0.12078857421875|unsuper_loss: 0.0
average reward score: 7.25390625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.48%) |Training time=1.38s (46.18%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.88 |AvgSamplesPerSec=43.38
epoch: 0|step: 44|ppo_ep: 1|act_loss: 0.209716796875|cri_loss: 0.1561279296875|unsuper_loss: 0.0
average reward score: 7.1328125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.57%) |Training time=1.26s (43.93%) |Others=0.24 (8.50%)|CurSamplesPerSec=44.70 |AvgSamplesPerSec=43.41
epoch: 0|step: 45|ppo_ep: 1|act_loss: 0.235595703125|cri_loss: 0.175048828125|unsuper_loss: 0.0
average reward score: 7.4609375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.55%) |Training time=1.37s (46.10%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.95 |AvgSamplesPerSec=43.40
epoch: 0|step: 46|ppo_ep: 1|act_loss: 0.150634765625|cri_loss: 0.1077880859375|unsuper_loss: 0.0
average reward score: 7.53125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.52%) |Training time=1.26s (43.97%) |Others=0.24 (8.51%)|CurSamplesPerSec=44.78 |AvgSamplesPerSec=43.43
epoch: 0|step: 47|ppo_ep: 1|act_loss: 0.265869140625|cri_loss: 0.186279296875|unsuper_loss: 0.0
average reward score: 7.0078125
-------------------------------------------------------------------------------------
|E2E latency=3.01s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.17%) |Training time=1.38s (45.76%) |Others=0.27 (9.07%)|CurSamplesPerSec=42.57 |AvgSamplesPerSec=43.41
epoch: 0|step: 48|ppo_ep: 1|act_loss: -0.04534912109375|cri_loss: 0.0047607421875|unsuper_loss: 0.0
average reward score: 7.390625
-------------------------------------------------------------------------------------
|E2E latency=3.77s |Gather latency=0.00s (0.00%) |Generate time=1.36s (35.97%) |Training time=1.26s (33.42%) |Others=1.15 (30.60%)|CurSamplesPerSec=33.98 |AvgSamplesPerSec=43.17
epoch: 0|step: 49|ppo_ep: 1|act_loss: -0.042724609375|cri_loss: 0.004852294921875|unsuper_loss: 0.0
average reward score: 7.57421875
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.55%) |Training time=1.37s (46.10%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.00 |AvgSamplesPerSec=43.16
epoch: 0|step: 50|ppo_ep: 1|act_loss: -0.0849609375|cri_loss: -0.012115478515625|unsuper_loss: 0.0
average reward score: 7.30078125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.29%) |Training time=1.26s (43.84%) |Others=0.25 (8.86%)|CurSamplesPerSec=44.69 |AvgSamplesPerSec=43.19
epoch: 0|step: 51|ppo_ep: 1|act_loss: -0.01458740234375|cri_loss: 0.041717529296875|unsuper_loss: 0.0
average reward score: 7.06640625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.39%) |Training time=1.38s (46.26%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.89 |AvgSamplesPerSec=43.19
epoch: 0|step: 52|ppo_ep: 1|act_loss: 0.001190185546875|cri_loss: 0.0278778076171875|unsuper_loss: 0.0
average reward score: 7.3515625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.42%) |Training time=1.26s (44.05%) |Others=0.24 (8.53%)|CurSamplesPerSec=44.83 |AvgSamplesPerSec=43.22
epoch: 0|step: 53|ppo_ep: 1|act_loss: -0.076171875|cri_loss: -0.009063720703125|unsuper_loss: 0.0
average reward score: 7.37890625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.56%) |Training time=1.37s (46.10%) |Others=0.25 (8.33%)|CurSamplesPerSec=42.95 |AvgSamplesPerSec=43.21
epoch: 0|step: 54|ppo_ep: 1|act_loss: -0.0325927734375|cri_loss: 0.017486572265625|unsuper_loss: 0.0
average reward score: 7.25390625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.46%) |Training time=1.26s (43.96%) |Others=0.25 (8.59%)|CurSamplesPerSec=44.80 |AvgSamplesPerSec=43.24
epoch: 0|step: 55|ppo_ep: 1|act_loss: -0.045379638671875|cri_loss: 0.00115966796875|unsuper_loss: 0.0
average reward score: 7.03515625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.51%) |Training time=1.37s (46.15%) |Others=0.25 (8.33%)|CurSamplesPerSec=42.99 |AvgSamplesPerSec=43.23
epoch: 0|step: 56|ppo_ep: 1|act_loss: -0.06561279296875|cri_loss: -0.0035400390625|unsuper_loss: 0.0
average reward score: 7.47265625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.38%) |Training time=1.25s (43.91%) |Others=0.25 (8.71%)|CurSamplesPerSec=44.80 |AvgSamplesPerSec=43.26
epoch: 0|step: 57|ppo_ep: 1|act_loss: -0.020538330078125|cri_loss: 0.017822265625|unsuper_loss: 0.0
average reward score: 7.3828125
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.71%) |Training time=1.37s (45.95%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.04 |AvgSamplesPerSec=43.26
epoch: 0|step: 58|ppo_ep: 1|act_loss: -0.0457763671875|cri_loss: 0.01409912109375|unsuper_loss: 0.0
average reward score: 7.08984375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.58%) |Training time=1.26s (44.01%) |Others=0.24 (8.40%)|CurSamplesPerSec=44.82 |AvgSamplesPerSec=43.28
[2023-04-21 15:34:39,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[2.8950000000000002e-06, 2.8950000000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:34:39,714] [INFO] [timer.py:199:stop] epoch=0/micro_step=60/global_step=30, RunningAvgSamplesPerSec=164.0956187238713, CurrSamplesPerSec=163.21779690246365, MemAllocated=16.18GB, MaxMemAllocated=33.09GB
[2023-04-21 15:34:39,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=5, lr=[1.25e-06, 1.25e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
epoch: 0|step: 59|ppo_ep: 1|act_loss: -0.0928955078125|cri_loss: -0.016815185546875|unsuper_loss: 0.0
average reward score: 7.16015625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.42%) |Training time=1.38s (46.28%) |Others=0.25 (8.31%)|CurSamplesPerSec=42.91 |AvgSamplesPerSec=43.28
epoch: 0|step: 60|ppo_ep: 1|act_loss: -0.09613037109375|cri_loss: -0.0184326171875|unsuper_loss: 0.0
average reward score: 7.0859375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.48%) |Training time=1.26s (43.93%) |Others=0.25 (8.59%)|CurSamplesPerSec=44.76 |AvgSamplesPerSec=43.30
epoch: 0|step: 61|ppo_ep: 1|act_loss: -0.043670654296875|cri_loss: 0.00286865234375|unsuper_loss: 0.0
average reward score: 7.328125
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.59%) |Training time=1.37s (46.04%) |Others=0.25 (8.37%)|CurSamplesPerSec=43.05 |AvgSamplesPerSec=43.30
epoch: 0|step: 62|ppo_ep: 1|act_loss: -0.0701904296875|cri_loss: -0.010894775390625|unsuper_loss: 0.0
average reward score: 7.390625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.40%) |Training time=1.26s (44.00%) |Others=0.25 (8.60%)|CurSamplesPerSec=44.79 |AvgSamplesPerSec=43.32
epoch: 0|step: 63|ppo_ep: 1|act_loss: -0.05963134765625|cri_loss: 0.00048828125|unsuper_loss: 0.0
average reward score: 7.296875
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.51%) |Training time=1.38s (46.17%) |Others=0.25 (8.32%)|CurSamplesPerSec=42.94 |AvgSamplesPerSec=43.31
epoch: 0|step: 64|ppo_ep: 1|act_loss: -0.049560546875|cri_loss: -0.01373291015625|unsuper_loss: 0.0
average reward score: 7.58984375
-------------------------------------------------------------------------------------
|E2E latency=2.87s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.43%) |Training time=1.26s (43.86%) |Others=0.25 (8.71%)|CurSamplesPerSec=44.67 |AvgSamplesPerSec=43.33
epoch: 0|step: 65|ppo_ep: 1|act_loss: -0.0771484375|cri_loss: -0.021087646484375|unsuper_loss: 0.0
average reward score: 7.3203125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.67%) |Training time=1.37s (45.97%) |Others=0.25 (8.36%)|CurSamplesPerSec=42.98 |AvgSamplesPerSec=43.33
epoch: 0|step: 66|ppo_ep: 1|act_loss: -0.031646728515625|cri_loss: 0.003753662109375|unsuper_loss: 0.0
average reward score: 7.20703125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.45%) |Training time=1.26s (43.93%) |Others=0.25 (8.62%)|CurSamplesPerSec=44.74 |AvgSamplesPerSec=43.35
epoch: 0|step: 67|ppo_ep: 1|act_loss: -0.050933837890625|cri_loss: -0.001556396484375|unsuper_loss: 0.0
average reward score: 7.2109375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.48%) |Training time=1.38s (46.18%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.92 |AvgSamplesPerSec=43.34
epoch: 0|step: 68|ppo_ep: 1|act_loss: -0.05072021484375|cri_loss: -0.0010986328125|unsuper_loss: 0.0
average reward score: 7.45703125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.37%) |Training time=1.26s (43.93%) |Others=0.25 (8.71%)|CurSamplesPerSec=44.73 |AvgSamplesPerSec=43.36
epoch: 0|step: 69|ppo_ep: 1|act_loss: -0.080810546875|cri_loss: -0.018951416015625|unsuper_loss: 0.0
average reward score: 7.16796875
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.53%) |Training time=1.38s (46.11%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.92 |AvgSamplesPerSec=43.36
epoch: 0|step: 70|ppo_ep: 1|act_loss: -0.0191650390625|cri_loss: 0.0068359375|unsuper_loss: 0.0
average reward score: 7.40625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.47%) |Training time=1.26s (43.93%) |Others=0.25 (8.60%)|CurSamplesPerSec=44.71 |AvgSamplesPerSec=43.37
epoch: 0|step: 71|ppo_ep: 1|act_loss: -0.00933837890625|cri_loss: 0.0135650634765625|unsuper_loss: 0.0
average reward score: 7.28515625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.54%) |Training time=1.37s (46.13%) |Others=0.25 (8.32%)|CurSamplesPerSec=42.96 |AvgSamplesPerSec=43.37
epoch: 0|step: 72|ppo_ep: 1|act_loss: 0.0244293212890625|cri_loss: 0.03143310546875|unsuper_loss: 0.0
average reward score: 7.4375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.51%) |Training time=1.26s (44.01%) |Others=0.24 (8.48%)|CurSamplesPerSec=44.82 |AvgSamplesPerSec=43.39
epoch: 0|step: 73|ppo_ep: 1|act_loss: 0.03863525390625|cri_loss: 0.03424072265625|unsuper_loss: 0.0
average reward score: 7.59375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.45%) |Training time=1.38s (46.21%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.91 |AvgSamplesPerSec=43.38
epoch: 0|step: 74|ppo_ep: 1|act_loss: 0.09228515625|cri_loss: 0.059326171875|unsuper_loss: 0.0
average reward score: 7.38671875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.37%) |Training time=1.26s (44.04%) |Others=0.25 (8.59%)|CurSamplesPerSec=44.77 |AvgSamplesPerSec=43.40
epoch: 0|step: 75|ppo_ep: 1|act_loss: 0.0281219482421875|cri_loss: 0.0290374755859375|unsuper_loss: 0.0
average reward score: 7.30859375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.52%) |Training time=1.38s (46.14%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.91 |AvgSamplesPerSec=43.39
epoch: 0|step: 76|ppo_ep: 1|act_loss: 0.10906982421875|cri_loss: 0.0718994140625|unsuper_loss: 0.0
average reward score: 7.015625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.31%) |Training time=1.26s (43.87%) |Others=0.25 (8.81%)|CurSamplesPerSec=44.69 |AvgSamplesPerSec=43.41
epoch: 0|step: 77|ppo_ep: 1|act_loss: 0.04742431640625|cri_loss: 0.037567138671875|unsuper_loss: 0.0
average reward score: 7.44140625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.41%) |Training time=1.38s (46.27%) |Others=0.25 (8.32%)|CurSamplesPerSec=42.89 |AvgSamplesPerSec=43.40
epoch: 0|step: 78|ppo_ep: 1|act_loss: 0.07208251953125|cri_loss: 0.05035400390625|unsuper_loss: 0.0
average reward score: 7.296875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.35s (47.33%) |Training time=1.26s (43.98%) |Others=0.25 (8.69%)|CurSamplesPerSec=44.73 |AvgSamplesPerSec=43.42
[2023-04-21 15:35:38,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[3.86e-06, 3.86e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:35:38,132] [INFO] [timer.py:199:stop] epoch=0/micro_step=80/global_step=40, RunningAvgSamplesPerSec=163.9638119135725, CurrSamplesPerSec=163.15976102440342, MemAllocated=16.18GB, MaxMemAllocated=33.09GB
[2023-04-21 15:35:38,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=5, lr=[1.75e-06, 1.75e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
epoch: 0|step: 79|ppo_ep: 1|act_loss: 0.031707763671875|cri_loss: 0.0237274169921875|unsuper_loss: 0.0
average reward score: 7.55859375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.43%) |Training time=1.38s (46.24%) |Others=0.25 (8.33%)|CurSamplesPerSec=42.93 |AvgSamplesPerSec=43.41
epoch: 0|step: 80|ppo_ep: 1|act_loss: 0.10638427734375|cri_loss: 0.0684814453125|unsuper_loss: 0.0
average reward score: 7.390625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.41%) |Training time=1.26s (43.98%) |Others=0.25 (8.61%)|CurSamplesPerSec=44.69 |AvgSamplesPerSec=43.43
epoch: 0|step: 81|ppo_ep: 1|act_loss: 0.06353759765625|cri_loss: 0.039764404296875|unsuper_loss: 0.0
average reward score: 7.3515625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.51%) |Training time=1.38s (46.15%) |Others=0.25 (8.33%)|CurSamplesPerSec=42.95 |AvgSamplesPerSec=43.42
epoch: 0|step: 82|ppo_ep: 1|act_loss: 0.1021728515625|cri_loss: 0.062255859375|unsuper_loss: 0.0
average reward score: 7.33203125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.48%) |Training time=1.26s (44.02%) |Others=0.24 (8.50%)|CurSamplesPerSec=44.78 |AvgSamplesPerSec=43.44
epoch: 0|step: 83|ppo_ep: 1|act_loss: 0.07415771484375|cri_loss: 0.04986572265625|unsuper_loss: 0.0
average reward score: 7.53515625
-------------------------------------------------------------------------------------
|E2E latency=2.99s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.40%) |Training time=1.38s (46.26%) |Others=0.25 (8.34%)|CurSamplesPerSec=42.87 |AvgSamplesPerSec=43.43
epoch: 0|step: 84|ppo_ep: 1|act_loss: 0.0263519287109375|cri_loss: 0.01934814453125|unsuper_loss: 0.0
average reward score: 7.41796875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.42%) |Training time=1.26s (43.91%) |Others=0.25 (8.66%)|CurSamplesPerSec=44.68 |AvgSamplesPerSec=43.44
epoch: 0|step: 85|ppo_ep: 1|act_loss: -0.00276947021484375|cri_loss: 0.003330230712890625|unsuper_loss: 0.0
average reward score: 7.40234375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.49%) |Training time=1.38s (46.17%) |Others=0.25 (8.33%)|CurSamplesPerSec=42.91 |AvgSamplesPerSec=43.44
epoch: 0|step: 86|ppo_ep: 1|act_loss: 0.004863739013671875|cri_loss: 0.0105133056640625|unsuper_loss: 0.0
average reward score: 7.5859375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.55%) |Training time=1.26s (44.02%) |Others=0.24 (8.42%)|CurSamplesPerSec=44.82 |AvgSamplesPerSec=43.45
epoch: 0|step: 87|ppo_ep: 1|act_loss: 0.034393310546875|cri_loss: 0.024261474609375|unsuper_loss: 0.0
average reward score: 7.19921875
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.49%) |Training time=1.37s (46.14%) |Others=0.25 (8.36%)|CurSamplesPerSec=43.01 |AvgSamplesPerSec=43.45
epoch: 0|step: 88|ppo_ep: 1|act_loss: -0.023284912109375|cri_loss: -0.00608062744140625|unsuper_loss: 0.0
average reward score: 7.5546875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.53%) |Training time=1.26s (44.01%) |Others=0.24 (8.47%)|CurSamplesPerSec=44.82 |AvgSamplesPerSec=43.46
epoch: 0|step: 89|ppo_ep: 1|act_loss: -0.012969970703125|cri_loss: 0.00196075439453125|unsuper_loss: 0.0
average reward score: 7.15234375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.49%) |Training time=1.38s (46.16%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.96 |AvgSamplesPerSec=43.46
epoch: 0|step: 90|ppo_ep: 1|act_loss: 0.01551055908203125|cri_loss: 0.01326751708984375|unsuper_loss: 0.0
average reward score: 7.6328125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.61%) |Training time=1.26s (44.09%) |Others=0.24 (8.29%)|CurSamplesPerSec=44.81 |AvgSamplesPerSec=43.47
epoch: 0|step: 91|ppo_ep: 1|act_loss: -0.0266571044921875|cri_loss: -0.00677490234375|unsuper_loss: 0.0
average reward score: 7.33984375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.63%) |Training time=1.37s (46.02%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.00 |AvgSamplesPerSec=43.47
epoch: 0|step: 92|ppo_ep: 1|act_loss: -0.046875|cri_loss: -0.018218994140625|unsuper_loss: 0.0
average reward score: 7.65625
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.57%) |Training time=1.26s (44.10%) |Others=0.24 (8.32%)|CurSamplesPerSec=44.84 |AvgSamplesPerSec=43.48
epoch: 0|step: 93|ppo_ep: 1|act_loss: -0.04583740234375|cri_loss: -0.016265869140625|unsuper_loss: 0.0
average reward score: 7.3515625
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.69%) |Training time=1.37s (45.96%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.03 |AvgSamplesPerSec=43.48
epoch: 0|step: 94|ppo_ep: 1|act_loss: 0.0062103271484375|cri_loss: 0.00945281982421875|unsuper_loss: 0.0
average reward score: 7.34765625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.56%) |Training time=1.26s (43.95%) |Others=0.24 (8.50%)|CurSamplesPerSec=44.81 |AvgSamplesPerSec=43.49
epoch: 0|step: 95|ppo_ep: 1|act_loss: -0.0054779052734375|cri_loss: 0.001750946044921875|unsuper_loss: 0.0
average reward score: 7.40625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.51%) |Training time=1.37s (46.12%) |Others=0.25 (8.37%)|CurSamplesPerSec=42.96 |AvgSamplesPerSec=43.48
epoch: 0|step: 96|ppo_ep: 1|act_loss: -0.00698089599609375|cri_loss: 0.0005340576171875|unsuper_loss: 0.0
average reward score: 7.390625
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.71%) |Training time=1.26s (44.08%) |Others=0.23 (8.20%)|CurSamplesPerSec=44.88 |AvgSamplesPerSec=43.50
epoch: 0|step: 97|ppo_ep: 1|act_loss: -0.021575927734375|cri_loss: -0.00749969482421875|unsuper_loss: 0.0
average reward score: 7.6328125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.54%) |Training time=1.37s (46.11%) |Others=0.25 (8.36%)|CurSamplesPerSec=42.96 |AvgSamplesPerSec=43.49
epoch: 0|step: 98|ppo_ep: 1|act_loss: 0.010406494140625|cri_loss: 0.007537841796875|unsuper_loss: 0.0
average reward score: 7.30078125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.56%) |Training time=1.26s (44.15%) |Others=0.24 (8.30%)|CurSamplesPerSec=44.83 |AvgSamplesPerSec=43.51
[2023-04-21 15:36:36,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[4.825e-06, 4.825e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:36:36,500] [INFO] [timer.py:199:stop] epoch=0/micro_step=100/global_step=50, RunningAvgSamplesPerSec=163.91644763294437, CurrSamplesPerSec=163.85420123081957, MemAllocated=16.18GB, MaxMemAllocated=33.09GB
[2023-04-21 15:36:36,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=5, lr=[2.25e-06, 2.25e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
epoch: 0|step: 99|ppo_ep: 1|act_loss: 0.025299072265625|cri_loss: 0.01544952392578125|unsuper_loss: 0.0
average reward score: 7.65625
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.59%) |Training time=1.37s (46.06%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.99 |AvgSamplesPerSec=43.50
epoch: 0|step: 100|ppo_ep: 1|act_loss: 0.007808685302734375|cri_loss: 0.0066680908203125|unsuper_loss: 0.0
average reward score: 7.48828125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.44%) |Training time=1.26s (44.04%) |Others=0.24 (8.52%)|CurSamplesPerSec=44.76 |AvgSamplesPerSec=43.51
epoch: 0|step: 101|ppo_ep: 1|act_loss: 0.025665283203125|cri_loss: 0.016571044921875|unsuper_loss: 0.0
average reward score: 7.66796875
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.55%) |Training time=1.37s (46.10%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.91 |AvgSamplesPerSec=43.51
epoch: 0|step: 102|ppo_ep: 1|act_loss: 0.029052734375|cri_loss: 0.0180206298828125|unsuper_loss: 0.0
average reward score: 7.375
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.63%) |Training time=1.26s (44.03%) |Others=0.24 (8.34%)|CurSamplesPerSec=44.87 |AvgSamplesPerSec=43.52
epoch: 0|step: 103|ppo_ep: 1|act_loss: 0.024017333984375|cri_loss: 0.0146636962890625|unsuper_loss: 0.0
average reward score: 7.51953125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.64%) |Training time=1.37s (45.99%) |Others=0.25 (8.37%)|CurSamplesPerSec=43.02 |AvgSamplesPerSec=43.51
epoch: 0|step: 104|ppo_ep: 1|act_loss: 0.024017333984375|cri_loss: 0.0144195556640625|unsuper_loss: 0.0
average reward score: 7.34375
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.52%) |Training time=1.26s (43.95%) |Others=0.24 (8.53%)|CurSamplesPerSec=44.80 |AvgSamplesPerSec=43.53
epoch: 0|step: 105|ppo_ep: 1|act_loss: 0.017425537109375|cri_loss: 0.01119232177734375|unsuper_loss: 0.0
average reward score: 7.51953125
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.61%) |Training time=1.37s (46.02%) |Others=0.25 (8.37%)|CurSamplesPerSec=43.04 |AvgSamplesPerSec=43.52
epoch: 0|step: 106|ppo_ep: 1|act_loss: 0.0248870849609375|cri_loss: 0.0164794921875|unsuper_loss: 0.0
average reward score: 7.2890625
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.60%) |Training time=1.26s (44.02%) |Others=0.24 (8.38%)|CurSamplesPerSec=44.84 |AvgSamplesPerSec=43.53
epoch: 0|step: 107|ppo_ep: 1|act_loss: -0.0013828277587890625|cri_loss: 0.001140594482421875|unsuper_loss: 0.0
average reward score: 7.59375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.63%) |Training time=1.37s (46.02%) |Others=0.25 (8.35%)|CurSamplesPerSec=42.99 |AvgSamplesPerSec=43.53
epoch: 0|step: 108|ppo_ep: 1|act_loss: 0.0001068115234375|cri_loss: 0.0017995834350585938|unsuper_loss: 0.0
average reward score: 7.453125
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.60%) |Training time=1.26s (44.09%) |Others=0.24 (8.30%)|CurSamplesPerSec=44.86 |AvgSamplesPerSec=43.54
epoch: 0|step: 109|ppo_ep: 1|act_loss: -0.0169219970703125|cri_loss: -0.0068511962890625|unsuper_loss: 0.0
average reward score: 7.5703125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.59%) |Training time=1.37s (46.04%) |Others=0.25 (8.37%)|CurSamplesPerSec=42.95 |AvgSamplesPerSec=43.54
epoch: 0|step: 110|ppo_ep: 1|act_loss: -0.01511383056640625|cri_loss: -0.006103515625|unsuper_loss: 0.0
average reward score: 7.37890625
-------------------------------------------------------------------------------------
|E2E latency=2.85s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.54%) |Training time=1.26s (44.06%) |Others=0.24 (8.39%)|CurSamplesPerSec=44.85 |AvgSamplesPerSec=43.55
epoch: 0|step: 111|ppo_ep: 1|act_loss: -0.0184783935546875|cri_loss: -0.0079803466796875|unsuper_loss: 0.0
average reward score: 7.68359375
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.35s (45.52%) |Training time=1.37s (46.11%) |Others=0.25 (8.37%)|CurSamplesPerSec=43.01 |AvgSamplesPerSec=43.54
epoch: 0|step: 112|ppo_ep: 1|act_loss: 0.001766204833984375|cri_loss: 0.0021648406982421875|unsuper_loss: 0.0
average reward score: 7.640625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.53%) |Training time=1.26s (43.97%) |Others=0.24 (8.50%)|CurSamplesPerSec=44.78 |AvgSamplesPerSec=43.55
epoch: 0|step: 113|ppo_ep: 1|act_loss: -0.0214996337890625|cri_loss: -0.00960540771484375|unsuper_loss: 0.0
average reward score: 7.39453125
-------------------------------------------------------------------------------------
|E2E latency=2.98s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.58%) |Training time=1.37s (46.07%) |Others=0.25 (8.35%)|CurSamplesPerSec=43.01 |AvgSamplesPerSec=43.55
epoch: 0|step: 114|ppo_ep: 1|act_loss: 0.0125732421875|cri_loss: 0.00782012939453125|unsuper_loss: 0.0
average reward score: 7.76953125
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.57%) |Training time=1.26s (43.90%) |Others=0.24 (8.53%)|CurSamplesPerSec=44.75 |AvgSamplesPerSec=43.56
epoch: 0|step: 115|ppo_ep: 1|act_loss: -0.00975799560546875|cri_loss: -0.00403594970703125|unsuper_loss: 0.0
average reward score: 7.34765625
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.71%) |Training time=1.36s (45.92%) |Others=0.25 (8.37%)|CurSamplesPerSec=43.09 |AvgSamplesPerSec=43.55
epoch: 0|step: 116|ppo_ep: 1|act_loss: 0.02685546875|cri_loss: 0.0150909423828125|unsuper_loss: 0.0
average reward score: 7.41015625
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.38%) |Training time=1.26s (43.97%) |Others=0.25 (8.65%)|CurSamplesPerSec=44.72 |AvgSamplesPerSec=43.56
epoch: 0|step: 117|ppo_ep: 1|act_loss: 0.01690673828125|cri_loss: 0.00970458984375|unsuper_loss: 0.0
average reward score: 7.4140625
-------------------------------------------------------------------------------------
|E2E latency=2.97s |Gather latency=0.00s (0.00%) |Generate time=1.36s (45.79%) |Training time=1.36s (45.83%) |Others=0.25 (8.38%)|CurSamplesPerSec=43.10 |AvgSamplesPerSec=43.56
epoch: 0|step: 118|ppo_ep: 1|act_loss: 0.030029296875|cri_loss: 0.016448974609375|unsuper_loss: 0.0
average reward score: 7.5546875
-------------------------------------------------------------------------------------
|E2E latency=2.86s |Gather latency=0.00s (0.00%) |Generate time=1.36s (47.65%) |Training time=1.26s (44.02%) |Others=0.24 (8.33%)|CurSamplesPerSec=44.82 |AvgSamplesPerSec=43.57
[2023-04-21 15:37:34,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[5.7900000000000005e-06, 5.7900000000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[2023-04-21 15:37:34,394] [INFO] [timer.py:199:stop] epoch=0/micro_step=120/global_step=60, RunningAvgSamplesPerSec=164.99967254691006, CurrSamplesPerSec=255.35532628297557, MemAllocated=15.56GB, MaxMemAllocated=33.09GB
[2023-04-21 15:37:34,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=5, lr=[2.7500000000000004e-06, 2.7500000000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
epoch: 0|step: 119|ppo_ep: 1|act_loss: -0.0152435302734375|cri_loss: -0.007266998291015625|unsuper_loss: 0.0
average reward score: 7.453125
-------------------------------------------------------------------------------------
saving model ...
saving model ...saving model ...saving model ...
saving model ...saving model ...


saving model ...

saving model ...
[2023-04-21 15:37:36,773] [INFO] [launch.py:460:main] Process 476991 exits successfully.
[2023-04-21 15:37:36,774] [INFO] [launch.py:460:main] Process 476993 exits successfully.
[2023-04-21 15:37:36,774] [INFO] [launch.py:460:main] Process 476996 exits successfully.
[2023-04-21 15:37:36,774] [INFO] [launch.py:460:main] Process 476990 exits successfully.
[2023-04-21 15:37:36,774] [INFO] [launch.py:460:main] Process 476995 exits successfully.
[2023-04-21 15:37:36,774] [INFO] [launch.py:460:main] Process 476992 exits successfully.
[2023-04-21 15:37:36,774] [INFO] [launch.py:460:main] Process 476994 exits successfully.
[2023-04-21 15:37:42,780] [INFO] [launch.py:460:main] Process 476989 exits successfully.
